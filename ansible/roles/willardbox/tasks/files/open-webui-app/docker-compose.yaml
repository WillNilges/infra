services:
#  ollama:
#    container_name: ollama
#    image: docker.io/ollama/ollama
#    restart: always
#    volumes:
#      - "/home/wilnil/open-webui-app/ollama:/root/.ollama"
#    networks:
#      - open-webui
#    ports:
#      - "11434:11434"
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all 

  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:ollama
    restart: always
    volumes:
      - "/home/wilnil/open-webui-app/open-webui:/app/backend/data"
      - "/home/wilnil/open-webui-app/ollama:/root/.ollama"
    #environment:
    #  - "OLLAMA_BASE_URL=http://ollama"
    ports:
      - "3000:8080"
    expose:
      - 3000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 

# Bundled
#docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

# podman run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama docker.io/ollama/ollama
# podman run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

networks:
  open-webui:
